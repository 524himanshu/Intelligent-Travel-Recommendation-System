{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, functions, types\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=pyspark.SparkContext(appName=\"project\")\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df = pd.read_json('dataset/hotel_info.json')\n",
    "h1_df = spark.createDataFrame(h_df)\n",
    "h1_df.createOrReplaceTempView('h1_df')\n",
    "\n",
    "temp=spark.sql(\"SELECT df.id FROM (SELECT id, COUNT(*) as tot_count FROM h1_df GROUP BY id ORDER BY tot_count DESC) df WHERE df.tot_count>1\")\n",
    "temp.createOrReplaceTempView('temp')\n",
    "del_dup = spark.sql(\"SELECT h1_df.* FROM h1_df LEFT JOIN temp ON h1_df.id == temp.id WHERE temp.id IS NULL\")\n",
    "del_dup.createOrReplaceTempView('del_dup')\n",
    "\n",
    "del_dup = del_dup.withColumn(\"amenities\", functions.split(del_dup[\"amenities\"], \",\").cast(\"array<string>\"))\n",
    "del_dup.createOrReplaceTempView('del_dup')\n",
    "\n",
    "newh_df  = spark.sql(\"SELECT id,explode(amenities) as amenities FROM del_dup\")\n",
    "newh_df.createOrReplaceTempView('newh_df')\n",
    "\n",
    "newh1_df  = spark.sql(\"SELECT amenities,COUNT(amenities) AS tot_count FROM newh_df GROUP BY amenities ORDER BY tot_count DESC\")\n",
    "newh1_df.createOrReplaceTempView('newh1_df')\n",
    "# del_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+\n",
      "|  id|           amenities|ameni_len|\n",
      "+----+--------------------+---------+\n",
      "| 155|[['Pool',  'Free ...|        5|\n",
      "|5846|[['Pool',  'Free ...|        5|\n",
      "|1882|[['Restaurant',  ...|        5|\n",
      "|5871|[['Restaurant',  ...|        5|\n",
      "|1241|[['Pool',  'Free ...|        5|\n",
      "|5208|[['Pool',  'Free ...|        5|\n",
      "|5915|[['Pool',  'Room ...|        5|\n",
      "|4161|[['Pool',  'Free ...|        5|\n",
      "|1480|[['Pool',  'Free ...|        5|\n",
      "|1711|[['Pool',  'Free ...|        5|\n",
      "|2489|[['Pool',  'Free ...|        5|\n",
      "|2906|[['Pool',  'Free ...|        5|\n",
      "|  34|[['Pool',  'Free ...|        5|\n",
      "| 385|[['Pool',  'Free ...|        5|\n",
      "|3069|[['Pool',  'Free ...|        5|\n",
      "|3061|[['Restaurant',  ...|        5|\n",
      "|1055|[['Pool',  'Free ...|        5|\n",
      "|4519|[['Pool',  'Room ...|        5|\n",
      "| 938|[['Restaurant',  ...|        5|\n",
      "|  22|[['Pool',  'Room ...|        5|\n",
      "+----+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amenities_pref = [\" 'Non-smoking hotel'\",\" 'Air conditioning'\",\" 'Free parking'\",\" 'Pets Allowed ( Dog / Pet Friendly )'\",\" 'Free High Speed Internet (WiFi)'\"]\n",
    "\n",
    "pa_df = pd.DataFrame(amenities_pref,columns=[\"amenities_pref\"])\n",
    "\n",
    "a_df = spark.createDataFrame(pa_df)\n",
    "a_df.createOrReplaceTempView('a_df')\n",
    "\n",
    "newa_df  = spark.sql(\"SELECT * FROM newh_df INNER JOIN a_df WHERE newh_df.amenities=a_df.amenities_pref\")\n",
    "\n",
    "ameni_comb = newa_df.groupBy(functions.col(\"id\")).agg(functions.collect_list(functions.col(\"amenities\")).alias(\"amenities\"))\n",
    "amenities_len=ameni_comb.withColumn(\"ameni_len\",functions.size(ameni_comb[\"amenities\"])).orderBy(functions.col(\"ameni_len\"), ascending=False)\n",
    "amenities_len.createOrReplaceTempView(\"amenities_len\")\n",
    "\n",
    "ameni_df = spark.sql(\"SELECT a.id,h.amenities,a.ameni_len FROM del_dup h INNER JOIN amenities_len a WHERE h.id=a.id ORDER BY a.ameni_len DESC\")\n",
    "ameni_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+------+\n",
      "|  id|           amenities|ameni_len|rating|\n",
      "+----+--------------------+---------+------+\n",
      "|2983|[['Pool',  'Free ...|        5|     5|\n",
      "|2489|[['Pool',  'Free ...|        5|     5|\n",
      "|5846|[['Pool',  'Free ...|        5|     5|\n",
      "|1480|[['Pool',  'Free ...|        5|     5|\n",
      "|1711|[['Pool',  'Free ...|        5|     5|\n",
      "|5915|[['Pool',  'Room ...|        5|     5|\n",
      "|1241|[['Pool',  'Free ...|        5|     5|\n",
      "| 155|[['Pool',  'Free ...|        5|     5|\n",
      "|4161|[['Pool',  'Free ...|        5|     5|\n",
      "|5871|[['Restaurant',  ...|        5|     5|\n",
      "|  22|[['Pool',  'Room ...|        5|     5|\n",
      "|3061|[['Restaurant',  ...|        5|     5|\n",
      "|2906|[['Pool',  'Free ...|        5|     5|\n",
      "| 385|[['Pool',  'Free ...|        5|     5|\n",
      "|5208|[['Pool',  'Free ...|        5|     5|\n",
      "| 938|[['Restaurant',  ...|        5|     5|\n",
      "|1882|[['Restaurant',  ...|        5|     5|\n",
      "|4519|[['Pool',  'Room ...|        5|     5|\n",
      "|1055|[['Pool',  'Free ...|        5|     5|\n",
      "|3069|[['Pool',  'Free ...|        5|     5|\n",
      "+----+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_rating(x):\n",
    "    val = x / 5\n",
    "    if x >= 0 and x <= val:\n",
    "        return 1\n",
    "    elif x > val and x <= 2*val:\n",
    "        return 2\n",
    "    elif x > 2*val and x <= 3*val:\n",
    "        return 3\n",
    "    elif x > 3*val and x <= 4*val:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "find_rating = functions.udf(lambda a: get_rating(a), types.IntegerType())\n",
    "\n",
    "usr_rating = ameni_df.withColumn(\"rating\",find_rating(\"ameni_len\"))\n",
    "usr_rating.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|att_id|\n",
      "+----+------+\n",
      "| 265|   202|\n",
      "| 315|   233|\n",
      "| 340|   244|\n",
      "| 460|   332|\n",
      "| 480|   347|\n",
      "| 830|   613|\n",
      "| 835|   617|\n",
      "| 845|   623|\n",
      "| 955|   694|\n",
      "|1210|   873|\n",
      "|1215|   875|\n",
      "|1275|   925|\n",
      "|1301|   946|\n",
      "|1308|   951|\n",
      "|1383|  1008|\n",
      "|1445|  1040|\n",
      "|1625|  1166|\n",
      "|1790|  1312|\n",
      "|1815|  1334|\n",
      "|1980|  1459|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('dataset/reviews.json')\n",
    "# df=df.iloc[:500,:]\n",
    "\n",
    "# df[\"user_id\"]=df.user_name.astype(\"category\").cat.codes\n",
    "df[\"att_id\"]=df.id.astype('category').cat.codes\n",
    "# df.head()\n",
    "\n",
    "rev_df = spark.createDataFrame(df)\n",
    "rev_df.createOrReplaceTempView('rev_df')\n",
    "\n",
    "rev_temp=spark.sql(\"SELECT df.id FROM (SELECT id, COUNT(*) as tot_count FROM rev_df GROUP BY id ORDER BY tot_count DESC) df WHERE df.tot_count>1\")\n",
    "rev_temp.createOrReplaceTempView('rev_temp')\n",
    "\n",
    "s_df = spark.sql(\"SELECT rev_df.* FROM rev_df LEFT JOIN rev_temp ON rev_df.id == rev_temp.id WHERE rev_temp.id IS NULL\")\n",
    "s_df.createOrReplaceTempView('s_df')\n",
    "\n",
    "# s_df.select(\"user_name\").distinct().count()\n",
    "\n",
    "#String Indexing user_name \n",
    "indexer = StringIndexer(inputCol=\"user_name\", outputCol=\"user_id\")\n",
    "indexed = indexer.fit(s_df).transform(s_df)\n",
    "u_id_df = indexed.withColumn(\"user_id\",indexed[\"user_id\"].cast(\"Int\"))\n",
    "u_id_df.createOrReplaceTempView('u_id_df')\n",
    "\n",
    "uid_count = u_id_df.select(\"user_id\").distinct().count()\n",
    "\n",
    "# String Indexing id \n",
    "# str_in = StringIndexer(inputCol=\"id\", outputCol=\"att_id\",stringOrderType='frequencyAsc')\n",
    "# att_indexed = str_in.fit(u_id_df).transform(u_id_df)\n",
    "# att_id_df = att_indexed.withColumn(\"att_id\",indexed[\"id\"].cast(\"Int\"))\n",
    "# att_id_df.createOrReplaceTempView('att_id_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "usrid_df = usr_rating.withColumn(\"usr_id\", functions.lit(uid_count))\n",
    "# usrid_df.show()\n",
    "\n",
    "# comb_df = u_id_df.union(usrid_df)\n",
    "# comb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(training,test)=s_df.randomSplit([0.8,0.2])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranks=[4,8,12]\n",
    "error = 20000\n",
    "for i in ranks:\n",
    "    als = ALS(maxIter=5,regParam=0.01,rank=i,userCol=\"user_id\",itemCol=\"att_id\",ratingCol=\"user_rating\",coldStartStrategy=\"drop\")\n",
    "    model = als.fit(training)\n",
    "    predictions = model.transform(test)\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\",labelCol=\"user_rating\",predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    if rmse < error:\n",
    "        model.write().overwrite().save(\"model_file\")\n",
    "        print(\"rank : \",i)\n",
    "        error = rmse        \n",
    "print(\"RMSE:\" +str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.createOrReplaceTempView('s_df')\n",
    "new_df  = spark.sql(\"SELECT DISTINCT att_id FROM s_df WHERE user_id != 13\").withColumn('user_id', functions.lit(13))\n",
    "\n",
    "als_model = ALSModel.load(\"model_file\")\n",
    "als_model.recommendForAllUsers(1).show()\n",
    "# predictions = als_model.transform(new_df)\n",
    "# predictions.show()\n",
    "# als_model.rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
